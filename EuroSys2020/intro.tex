%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Motivation:  spatial locality in KV-storage}
% KV-storage workloads} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% KV-stores are everywhere
Key-value stores (KV-stores) are widely used  by a broad range of applications and are projected
to continue to increase in popularity in years to come; market research  identifies them as the 
``driving factors'' of the NoSQL market, which is expected to garner \$4.2B by 2020~\cite{alliedmarketresearch}.

% In many applications, keys are composite
KV-stores provide a simple programming model. 
Data is an ordered collection of key-values pairs, and  the API supports random writes, 
random reads, and range queries. 

A common design pattern is the use of \emph{composite} keys that represent an agglomerate of attributes.
Typically, the first attribute -- called the \emph{primary key} -- has a skewed distribution, and so   access via the composite key exhibits \emph{spatial locality}, as 
popular primary keys result in popular \emph{key ranges}. 

One example of this arises in mobile analytics platforms, \inred{e.g., AppsFlyer~\cite{appsflyer}, Flurry~\cite{flurry}, 
and Google Firebase~\cite{firebase}. %~\cite{medium-mobile-analytics}.   
Such platforms help mobile developers to collect user activity data, analyze it, and optimize their app experience. 
They ingest massive streams of app event reports %(e.g., start, stop, or particular scenario within an app), 
in the real time, and provide a variety of insight queries into the data. For example, Flurry provided insights for  
1M+ apps across 2.6B user devices  in 2017~\cite{FlurryReport2017}. In order to support efficient analytics on 
per-app basis, such services aggregate data in KV-stores indexed by a composite key prefixed by unique app 
id. Other key attributes (timestamp, app version, user id, device model, location, event type, etc.) may vary
across the index tables.} 
%
We examine a trace of $200$ million events captured from a production mobile analytics engine over the course of four minutes.  
\Cref{fig:cdf} shows the access frequency distribution over the  $40$K app names occurring in this stream. \inred{It follows an  
explicit heavy-tail pattern:} $10$\% of the apps cover over $99.5$\% of the events; $1$\% of them  cover $94$\%; and fewer 
than $0.1$\% cover $70$\% of the events. 

% This means the data is highly skewed and with a very long tail (not depicted in the figure).
%Table~\ref {table:popular} shows the cumulative probability of the top-$20$ popular applications, covering more than $50$\% of the events. 
%With application name as the primary dimension of a composite key this distribution induces high spatial locality.


\begin{figure}[tb]
\centering
\includegraphics[width=0.7\columnwidth]{figs/cdf.pdf}
\caption{CDF of app names in a real-world mobile analytics event stream consisting of 200 million events.}
\label{fig:cdf}
\end{figure}

Composite keys arise in many additional domains, including messaging and social networks. 
For example a backend Facebook Messenger query may retrieve the last 100 messages for a 
given user~\cite{Borthakur:2011:AHG:1989323.1989438}. %, where the primary key is user id. 
In Facebook's social network, a graph edge is indexed by a key consisting of two 
object ids and an association type~\cite{Armstrong:2013:LDB:2463676.2465296}.
%Note further that 
Spatial locality   also arises with simple (non-composite) keys, for example, when 
reverse  URL domains are used as keys for web  indexing~\cite{Cho:1998:ECT:297805.297835}. 

% Overfitting for Zipf 
The prevalence of skewed  access  in real workloads is widely-recognized, 
and indeed standard benchmarks (e.g., YCSB~\cite{YCSB})  feature skewed key-access distributions like Zipf.
But these benchmarks fail to capture the spatial aspect of locality.
This, in turn, leads to storage systems being optimized for a skewed distribution on individual keys, % with no spatial locality,
e.g., by partitioning data by recent access time as opposed to by key.
In this work, we make spatial locality a first class consideration in KV-store design.
% which leads us to rethink the design principles underlying today's popular KV-stores.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Performance with spatial locality: the challenge }  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our main goal is to 
%draw attention to the importance of spatial locality in today's KV-store workloads and to 
put forth a KV-store design alternative  suited for the 
spatial locality arising in today's  workloads. 


% LSM is the standard 
The de facto standard approach used in high-throughput KV-stores today is \emph{LSM} (log-structured merge) trees~\cite{DBLP:journals/acta/ONeilCGO96}. 
The LSM design initially groups writes  into files \emph{temporally}, and not by key-range. 
A background \emph{compaction} process later merge-sorts any number of files, grouping data by keys. 

% LSM is  not ideal for spatial locality  because of  temporal grouping 
This approach is not ideal for workloads with high spatial locality for two reasons. 
First,  popular key ranges are fragmented across many files. 
Second,  compaction  is costly in terms of  both performance 
(disk bandwidth) and \emph{write amplification}, namely the number of physical writes 
associated with a single application write. The latter is  particularly important in SSDs as it increases disk wear. 
The temporal grouping means that compaction is indiscriminate with respect to key popularity:  
Since new (lower level) files are always merged with old (higher level) ones, 
 ``cold'' key ranges  continue to be repeatedly re-located by  compactions.  

% LSM is not the best when the entire working set fits in memory
Additionally, we note that LSM's temporal  organization optimizes disk I/O but induces a penalty on in-memory operation. 
All keys -- including popular ones -- are flushed to disk periodically, even though persistence is assured via a separate \emph{write-ahead-log (WAL)}.
Beyond increasing write amplification, this makes the flushed keys unavailable for fast read from memory,
which is  wasteful if the system incorporates sufficient DRAM to hold most of the active working set. 
The drop in DRAM prices (more than $6.6$x since 2010~\cite{dram-prices}) makes the latter scenario increasingly common.  

Yet  LSMs have supplanted the  traditional spatial data partitioning of B-trees for a reason~\cite{rocks-vs-inno}.
%A crucial challenge arising in spatially-organized  storage is how to persist all updates in a consistent way without sacrificing performance.
In  B-trees, each update induces a random I/O operation to a leaf, resulting in poor write performance.
Moreover, the need to preserve a consistent image of a leaf while it is being over-written induces high write amplification. 
$B^{\epsilon}$-trees~\cite{Brodal:2003:LBE:644108.644201} mitigate this cost using write buffers. % in internal tree nodes. 
However, this slows down lookups, which now have to search in %multiple 
unordered buffers. 
If the buffers reside on disk, the lookup time is unacceptably slow, whereas
if they are in RAM then they have to be complemented by a separate persistence mechanism, with its own costs.  
LSM's temporal organization overcomes these challenges by absorbing random writes in memory and periodically flushing them as 
sequential files to disk. 
Its resounding performance advantage has been repeatedly demonstrated, 
e.g., in a recent  study of the Percona mySQL server using three storage engines -- RocksDB, TokuDB, and InnoDB --
based on LSM, a B$^{\epsilon}$-tree, and a B-tree, respectively~\cite{toku-rocks-inno}.




\remove{
Obviously, we do not claim that spatial data partitioning is new; indeed, classical B-trees~\cite{DBLP:conf/sigmod/BayerM70,Comer79} 
 pre-date  LSM trees, and many B-tree variants~\cite{Brodal:2003:LBE:644108.644201,Bender15, Lehman:1981:ELC:319628.319663} have  emerged over the years. 
 However, it is important to note that these trees are  conceptual constructs rather than storage systems; 
 employing these concepts within a practical data store over a memory hierarchy 
 raises multiple challenges,  which perhaps explains their limited adoption in industrial KV-stores.
 A key challenge is what to persist when and in what format. 
 }
 
A second advantage of LSMs is ensuring consistency -- in particular, atomic scans --   under multi-threaded access. 
%This can be  tricky when a scan spans both memory-resident and disk-resident data.
LSMs provide atomic scans without locks using multi-versioning. 
In contrast, databases based on B- or  B$^\epsilon$-trees either use locks~\cite{innodblocking} 
or forgo scan consistency~\cite{tucana}.

%We are not aware of any published solution for atomic scans in storage based on  B- or  B$^\epsilon$-trees.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Our contribution: \sys}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Drum roll 
We present \sys, a high-throughput persistent KV-store geared towards spatial locality. 
\sys's  architecture (\cref{sec:principles}) 
combines a spatial data organization with LSM-like optimized batch I/O.
It is organized  as a list (rather than a tree) of large \emph{chunks} holding contiguous key ranges.
%
To achieve good performance with this data layout, we employ a number of mechanisms: 

\begin{enumerate} 
     \setlength{\itemindent}{-10pt}
\item
 a RAM cache of popular chunks, serving  reads and writes;  
\item
 a volatile index for direct access to chunks by key; 
 %-- either in RAM or via a single I/O operation -- to the chunk holding a given key;  
\item
 per-chunk WALs  transforming random I/O to sequential I/O at the chunk level, with in-memory compaction; 
\item
Bloom filters to expedite reads from disk; and 
\item 
%For the benefit of workloads that lack spatial locality, we further add  
a   cache for hot keys in workloads lacking spatial locality.
\end{enumerate}

Our algorithm (\cref{sec:design}) is designed for high %multi-threaded 
concurrency. 
%among threads running put, get, and scan operations, as well as background maintenance (compaction). 
It supports atomic scans using low-overhead multi-versioning, where versions are increased only by scans and not by updates. And it ensures consistency and correct recovery from failures. While the mechanisms we employ are all variants of ones appearing in the literature, their combination is novel and results in a high-performance storage system.





We implement \sys\ in  C++ (\cref{sec:impl}) and extensively evaluate it (\cref{sec:eval})
via three types of workloads: (1) a production trace collected from a large-scale mobile analytics platform; 
(2)  workloads with synthetically-generated composite keys exercising  standard YCSB scenarios~\cite{YCSB};
and (3)  YCSB's traditional benchmarks, which employ simple (non-composite) keys.  
%In all cases, 
We compare \sys\ to the recent (Oct 2018) release of RocksDB~\cite{RocksDB}, a mature industry-leading LSM KV-store. 
We  experimented with two additional open-source KV-stores, PebblesDB~\cite{PebblesDB} (a recent LSM KV-store) and  
% PreconaFT/
TokuDB~\cite{TokuDB} (the only publicly-available $B^{\epsilon}$-tree-based 
KV-store); both performed significantly worse than  RocksDB and \sys, so we focus on RocksDB results. 
%so we do not discuss them further.
%\inred{across the entire YCSB benchmark suite.} %, which is in line with previous studies of PerconaFT~\cite{tucana}, so we excluded them from further tests. 
Our main findings are: 
\begin{enumerate} 
  %   \setlength{\itemindent}{-10pt}
\item \sys\/ significantly outperforms RocksDB when spatial  locality is high.  
For instance, on a 256GB production dataset, \sys\ ingests data 3.7x faster than RocksDB,  scans recent  data up to 27\% faster, 
and reduces write amplification by nearly 4x. 
%And in large synthetic composite-key workloads, \sys\  improves over RocksDB's throughput by $24\% - 75\%$. 
\item \sys\/ significantly outperforms RocksDB whenever most of the working set fits in RAM, 
%For example, with synthetic composite keys and a memory-resident working set, \sys\  
accelerating scans by up to $3.5$x, puts by up to $2.3$x, and gets by up to $2$x. 
\item \sys\ is comparable to RocksDB in traditional YCSB workloads without spatial locality. 
\item \sys\ performs 20--25\% worse  than RocksDB  in mixed read/write workloads with large active working sets and no spatial locality. 
\item \sys\ has lower write amplification, especially on large datasets. \inred{Production trace?} 
\end{enumerate}

% Benefits
Our results underscore the advantage of \sys's spatially-organized chunks:
(1) there is little fragmentation of key ranges and hence better  performance under spatial locality; 
(2) in-memory chunk compaction saves disk flushes and reduces write volume;  
(3) keeping hot ranges in memory leads to better performance, especially when most of the working set fits in RAM.
In addition, in-chunk WALs  allow quick recovery from crashes with no need to replay any WAL.

% Downsides
The downside of spatial partitioning is that if the data lacks spatial locality and the active working set is big, 
caching an entire chunk for a single popular key is wasteful.  This design is less optimal for mixed read/write workloads lacking spatial locality, where the LSM approach may be more suitable.  

~\cref{sec:related}  surveys related work and~\cref{sec:conclusions} concludes this paper. 
