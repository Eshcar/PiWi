

%common LSM stores RocksDB, scylladb, HyperLevelDB, LevelDB, hbase, cassandra
The vast majority of industrial mainstream NoSQL KV-stores are  implemented as LSM trees~\cite{hbase, 
RocksDB, scylladb, Bigtable2008, cassandra2010}, building on the foundations set by O'Neil 
et al.~\cite{DBLP:journals/acta/ONeilCGO96, Muth1998}. 

Due to LSM's design popularity, much effort has been invested into working around its bottlenecks.
A variety of compaction strategies has been implemented in production systems~\cite{CallaghanCompaction, 
ScyllaCompaction} and research prototypes~\cite{triad, PebblesDB, vttrees, slmdb}.  Improvements
 include storage
optimizations~\cite{WiscKey, PebblesDB, vttrees, slmdb,Papagiannis:2018:EMK:3267809.3267824}, 
boosting  in-memory parallelism~\cite{scylladb, clsm2015}, and leveraging 
 workload redundancies to defer disk flushes~\cite{triad, accordion} or avoid re-writing sorted data~\cite{vttrees}. 
In contrast to these, \sys\/ eliminates the concept of temporally-organized levels altogether
and employs a flat  layout with in-memory compaction.

\remove{
A number of systems focus on reducing write amplification.
PebblesDB~\cite{PebblesDB} introduces fragmented LSM trees in which level files are 
sliced into {\em guards\/} of increasing granularity and organized in a skiplist-like layout. 
%This structure reduces write amplification. 
In contrast, \sys\/ eliminates the concept of levels altogether, 
and employs a flat  layout. WiscKey~\cite{WiscKey} separates key and value storage 
in SSTables, also in order to reduce amplification. This optimization is orthogonal to \sys's concepts,
and could benefit our work as well. 
%
VT-Trees~\cite{vttrees} apply stitching to avoid rewriting already sorted data. This improves performance and significantly reduces write amplification in some scenarios (e.g., time-series ingestion).
}

SLM-DB~\cite{slmdb} is an LSM design that relies on persistent memory for its mutable component and thus avoids WAL maintenance. It also re-designs the traditional LSM read path, utilizing a 
 B$^+$-tree index in persistent memory on top of a flat list of SSTables with LSM-like temporal data partitioning. In contrast, EvenDB does not rely on special hardware. 
 Moreover, to the best of our knowledge, SLM-DB does not support concurrent operations.

In-memory compaction has been recently implemented in HBase~\cite{accordion} by organizing
HBase's in-memory write store  as an LSM tree, eliminating redundancies 
in RAM to reduce disk flushes. However, being incremental to the LSM design, 
this approach fails to address spatial locality. 

Range-based  partitioning is also employed in  B-trees~\cite{Knuth:1998:ACP:280635} and variants like $B^{\epsilon}$-trees~\cite{Brodal:2003:LBE:644108.644201}.
 \cref{ssec:B-tree-compare} discusses the key challenges faced by storage systems adopting these designs, which 
 result in performance disadvantages compared to the LSM approach  (as shown, e.g., in~\cite{toku-rocks-inno}), 
 and the difficulty to support consistency  -- in particular, atomic scans --   under multi-threaded access.
 In contrast to B-tree nodes, \sys's chunks are not merely  a means to organize data on-disk -- they 
are also the basic units for DRAM caching, I/O-batching, logging, and compaction. 
This allows us to overcome the challenges of B- and $B^{\epsilon}$-trees via chunk-level logging with sequential I/O and 
in-memory compaction.
 
 \remove{
 , which suffer from a write bottleneck in random updates to 
leaf blocks. \sys\/ overcomes this limitation through (1) transforming random I/O to sequential I/O at the chunk level, 
(2) managing a write-through chunk cache in memory (munks), and (3) reducing I/O  through in-memory (munk) compaction. 

A $B^{\epsilon}$-tree~\cite{Brodal:2003:LBE:644108.644201} is a B-tree variant that uses overflow write buffers in internal nodes. 
This design speeds up writes and reduces write amplification, however lookups are slowed down by having to search in unordered 
buffers. $B^{\epsilon}$-trees have been used in KV-stores (TokuDB~\cite{TokuDB}) and filesystems (BetrFS~\cite{BetrFS}).  
}

Tucana~\cite{tucana} is an in-memory $B^{\epsilon}$-tree index over a persistent log of KV-pairs. It applies multiple system
optimizations to speed up I/O (all orthogonal to our work): block-device storage access, memory-mapped files, and copy-on-write 
on internal nodes. However, Tucana provids neither strong scan semantics nor consistent recovery, and does not support concurrent put operations.

A different line of work focuses on 
in-memory KV-stores~\cite{ignite, redis, memcached, Srinivasan:2016:AAR:3007263.3007276} providing fast volatile 
data storage, e.g., for web and application caches. Over time, many evolved to support durability,
%albeit as a second-class citizen in most cases. 
yet still require the complete data set to reside in memory.
%For example, Ignite~\cite{ignite} uses a B-tree index with in-place random updates. 
%These systems resemble \sys\/ in their memory-centric approach. 
Although \sys\ is optimized for ``sliding-local'' scenarios where the entire active working set is in memory at any given time, 
we do not expect all  the data to fit in memory. 

%Their persistent storage support is based on either B-tree~\cite{ignite} or LSM-tree design~\cite{redis}. 
%used for application data caching, but also as building blocks for distributed database with optional durability~\cite{ignite,redis}. These are not comparable with \sys\ as they are either not persistent~\cite{memcached}, do not support atomic scans~\cite{redis} or resemble  relational DBMS with a centralized WAL, , and in-place updates~\cite{ignite}  more than a KV LSM store.