

The API of \sys\ is presented in Section~\ref{ssec:api}. Section~\ref{ssec:layout} then discusses \sys's data 
organization and its evolution at run-time. The data structure's maintenance and 
synchronization among concurrent operations are discussed in Section~\ref{ssec:sync}.

\subsection{API and guarantees}
\label{ssec:api}

\sys\ is a persistent key-value store supporting \emph{put, get}, and atomic \emph{range scan} (or scan) operations. 
Scans are atomic in the sense that all values returned by a single scan belong to a consistent snapshot reflecting
the state of the data store at a unique point in time.

\Idit{More on API?}

\sys\ ensures \emph{durability} of all updates by writing updates to disk synchronously as part of the \emph{put} operation.

\subsection{Data organization and evolution}
\label{ssec:layout}

\begin{figure}[htb]
\centerline{
\includegraphics[width=\columnwidth]{PiWi.png}
}
\caption{\sys\ data layout.}
\label{fig:layout}
\end{figure}

\paragraph{Chunk-based layout.}

\sys's data layout is depicted in Figure~\ref{fig:layout}.
Similarly to BTrees \Idit{and other disk-friendly data structures?}, 
\sys\ organizes data in fixed-size \emph{chunks}, each holding a contiguous key range.
At run-time, a list of all chunks is kept in RAM, where each chunk's data 
(consisting of keys in the corresponding range and values associated with them) 
is kept on disk (for persistence), and possibly in memory (for fast access). 

More specifically, each chunk is associated with a \emph{file chunk}, or \emph{funk},
which is a persistent data structure consisting of three files holding the chunk's data -- a value array \emph{varray}, 
a sorted key array \emph{karray}, and a write buffer \emph{wbuf}. The vstore holds all the values associated with keys
in the chunk. When a funk is created, the kstore holds all the chunk's keys with pointers to corresponding values.
New keys are subsequently appended to the unsorted wbuf, while new values are appended to the chunk's vstore.
This structure allows us to update chunks without re-writing existing data, thus minimizing write amplification.

Since updates to disk are executed synchronously,  the data store reflecting all completed put operations 
can be consistently recovered from the on-disk funks at any time. \Idit{Need to discuss recovery somewhere.}

A subset of the chunks is also cached in memory to allow fast access, each in a data structure called \emph{memory chunk (munk)}. 
A munk consists of two arrays, kstore and vstore, where the kstore is organized as a sorted linked list. 
Updates are performed in place. Munks are volatile and can be removed and recreated from funks at any time.
Thus, multiple \emph{generations} of munks may exist for a chunk throughout its life time.

At run-time, \sys\ holds in memory a linked list of chunk objects representing all funks in the data store. 
Chunks are also indexed in-memory for fast access by key using a \emph{chunk index}, which is 
a sorted map (e.g., a sorted array, skip list, or search tree).
Note that since chunk objects do not hold actual keys and values, they are significantly smaller than munks and funks. 
\inred{A typical chunk object is smaller than 1KB, whereas the size of a funk or munk that holds 10K to 100K keys 
ranges between 1M to 100M depending on the data size. A typical \sys\ node holds thousands of chunks, with 
all chunk objects in memory in addition to hundreds of munks.} 

As key-value pairs are added, overwritten, and removed, munks and funks need to undergo reorganization. This includes  
\emph{compaction} to deallocate removed and overwritten data, 
\emph{sorting} keys to make searches more efficient,  
\emph{splitting} overflowing chunks.
\remove{, and \emph{merging} under-utilized ones.}
All reorganizations are performed by \sys's \emph{rebalance} operation.
If the chunk has a munk, then rebalance compacts and sorts the munk in-memory by creating a new 
(compacted and sorted) munk instead of the existing one. 
Funks of uncached chunks are also compacted by replacing them with new funks, albeit less frequently.
Splits and merges create new chunks as well as the munks and funks associated with them.

\paragraph{Multi-versioning.}

We support atomic scans via multi-versioning using a system-wide \emph{global version (GV)}. 
A scan operation creates a \emph{snapshot} associated with GV's current value by incrementing GV, 
which signals to ensuing put operations that they must not overwrite values associated with 
smaller versions than the new GV value.
This resembles a \emph{copy-on-write (CoW)} approach, which virtually creates a snapshot by 
indicating that data pertaining to the snapshot should not be modified in place.  
To allow garbage collection of old versions that are no longer required for any active scan, \sys\ also maintains 
a \emph{pending scan array (PSA)}, with one entry per active thread, tracking snapshot times of ongoing scans.

For linearizing updates, we associate each key-value pair written to the data store with a unique-per-key identifier.
This identifier is a tuple $\langle$ver, gen, i$\rangle$, where \emph{ver} is  the version read from GV 
(recall that GV is only incremented upon scans and hence might remain unchanged across multiple puts),
\emph{gen} is the generation of the last munk created in the corresponding chunk
(which may or may not still exist), 
and \emph{i} is a running sequence number of values inserted to the chunk in the current generation.
In case the munk exists, $i$ is the 
index of the cell holding the key in the munk's kstore as well as the cell holding the value in the munk's vstore. 

\paragraph{Chunk life cycle.}

A chunk can undergo two types of changes. The simple case occurs when the chunk's funk or munk is rebalanced,
and the more complicated  case occurs when the chunk is split, in which case it is replaced by two chunks in the chunks list.
In both cases, the chunk is immutable throughout the rebalance operation. 
In the simple case, the chunk's status changes to \emph{frozen} -- indicating that it is immutable -- 
when rebalance begins, and changes back to \emph{normal} when rebalance ends.  

\begin{figure}[htb]
\centerline{
\includegraphics[width=\columnwidth]{status.png}
}
\caption{Chunk lifecycle; immutable states are grey and mutable ones are white.}
\label{fig:status}
\end{figure}

In the second case, after the chunk is frozen, two new chunks are created. 
Since creating a new funk takes time, we first create two chunks pointing to the same funk, and allow the funk
creation process to execute in the background. 
When we replace the frozen chunk in the list with the two new ones, 
the old chunk is still accessible via the chunk index (even though it is no longer in the list). 
The new chunks are therefore created in \emph{infant} status, indicating that they are still immutable. 
Once the new chunks are indexed, the old chunk becomes \emph{expired}, and the status of the new chunks
changes to \emph{child}, indicating that it is no longer immutable, but shares a funk with another chunk,
which should be taken into account in future rebalances. Once the funk split completes, the chunk
status returns to normal. 
The chunk's life-cycle is depicted in Figure~\ref{fig:status}.

\paragraph{Data structure.}

The chunk data structure is given in Algorithm~\ref{alg:chunk}. 
In addition to its status, 
it holds a pointer to the appropriate funk, and, if applicable, also munk. 
It further keeps the generation number of its latest munk and the index,
which, in case there is an active munk, is the index of the next free cell in the munk's karray and varray.

The chunk additionally includes locks and data structures to synchronize concurrent access by multiple threads.
The replacement of a chunk (due to a split or a merge) or of a funk or munk associated with a given chunk 
must be executed atomically, and moreover, must be synchronized with concurrent puts. 
This is controlled by the chunk's rebalance\_lock, which is held for short time periods
during chunk/funk/munk replacements.  It is a shared/exclusive lock (r/w lock), acquired in shared mode 
by put operations and in exclusive mode by rebalance. Gets and scans do not need to acquire the lock at all,
and are completely wait-free.

To minimize I/O, we allow at most one thread to rebalance a funk at a given time; this is controlled by 
the  funk\_change\_lock. This lock is used at a coarse granularity -- it is held throughout the creation of the new funk. 
It is acquired using a try\_lock call, and threads that fail to acquire it do not retry, but instead wait for the winning thread 
to complete the funk's creation.
Finally, the chunk holds a data structure called PPA for synchronizing  puts with concurrent scans, as will be explained below. 

\begin{algorithm}[htb]
\begin{algorithmic}
\State status in $\{$infant, child, normal, frozen, expired$\}$
\State ptr funk \Comment funk disk address
\State ptr munk \Comment munk memory pointer
\State int gen \Comment munk generation
\State int i \Comment munk index
\State asymmetric lock rebalance\_lock \Comment r/w lock 
\State lock funk\_change\_lock \Comment acquired with try\_lock 
\State PPA[threads] \Comment pending put array
\end{algorithmic}
\caption{Chunk data structure.}
\label{alg:chunk}
\end{algorithm}



\subsection{Rebalancing and synchronization}
\label{ssec:sync}





