We extensively compare the \sys\/ prototype to RocksDB -- a mature industry-leading KV-store implementation  -- under a variety of scenarios. \remove{RocksDB is used as storage layer of multiple popular SQL and NoSQL databases, e.g., MyRocks~\cite{MyRocks} (incarnation of MySQL) and MongoRocks~\cite{MongoRocks} (incarnation of MongoDB). RocksDB is an LSM-tree that is highly optimized for both read and write scenarios. For example, its compaction scheduling policies are highly tuned to minimize impact on mainstream data access.} We use the most recent RocksDB release 5.17.2, available Oct 24, 2018.  

\subsection{Setup}
We employ a C++ implementation~\cite{Cpp-YCSB} of a widely popular YCSB~\cite{YCSB} benchmarking platform. 
YCSB provides a framework for KV-store comparisons, through a set of common data access API's and standard workloads. 
Most modern KV-stores implement  YCSB adapter API's. The platform decouples data access from workload generation, 
thereby providing common ground for backend comparison. A workload is characterized by a combination of get, put, 
and scan accesses, as well as by a synthetic distribution of keys and values. YCSB provides a set of benchmark workloads
inspired by real-life applications, and allows developing new ones through workload generation API's. 

A typical YCSB instance stress-tests the backend KV-store through a pool of concurrent worker threads that drive identical
workloads. It aggregates the key performance metrics for the scenario under test, e.g., total throughput and tail access latencies. 

\paragraph{Environment.}
Our experiment testbed is a 12-core Intel Xeon 5 machine with 4TB SSD disk. The driver application exercises 12 threads (maximum parallelism). In order to guarantee a fair memory allocation to all KV-stores, we run each experiment 
within a Linux container with 16 GB RAM. 

\paragraph{Data.} We use datasets of size 4 GB to 64 GB. Similarly to the published RocksDB benchmarks, we use 
10-byte keys that YCSB pads with a fixed 4-byte prefix (effectively, 14-byte keys), and 800-byte values. 

The dataset build stage sequentially fills an empty KV-store with the target number of KV-pairs. Following this, 
the evaluation stage performs 80 million data accesses, evenly spread across the worker threads.   

\paragraph{Workloads.} 

Our experiments leverage multiple workloads: 

\begin{enumerate}
\item {\em Zipf}. . The key frequencies are sampled from the heavy-tailed Zipf distribution, 
following the description in~\cite{?}, with $\theta = 0.8$. The keys themselves are sampled 
uniformly at random from the whole data range. This is a standard YCSB workload that captures
a multitude of use cases -- e.g., a web page cache distribution. 

\item {\em Zipf-range}. Similar to Zipf ($\theta=0.8$), except that we sample the key's $14$ most significant bits
(MSBs) rather than the complete key. The remainder of the key sampled uniformly at random. This workload
captures a multitude of mini-clusters of varying popularity -- for example, user posts in a social 
network~\cite{linkbench} or mobile app event logs~\cite{flurry}. In this setting, the key is structured
as a combination of the user (resp., app) id and the post (resp., event) id.  

\item {\em Latest}. New records are inserted, and the most recently inserted records are the 
most popular (e.g., status updates and reads). Also, a standard YCSB workload.  

\end{enumerate}

\paragraph{Configuration.} 

We set the \sys\/ hard chunk size limit to 10 MB, and the rebalance threshold to $0.7$ -- that is, 
most chunks are bound by 7 MB. 

We allocate a quota of 11 GB memory (out of the 16 GB container) to the munk cache and the row cache. 
For the Zipf and Latest workloads, 7 GB are given to munks. For the Zipf-range workloads, 9 GB 
are given to munks.  We limit the write buffer size to 2 MB for write-intensive workloads, and to $0.5$ MB 
for read intensive-workloads. 
  
We tune all RocksDB runtime parameters in accordance with its system performance guide~\cite{RocksDBPerf}.   

\subsection{Results}

\paragraph{YCSB-P (100\% put, Zipf and Zipf-prefix distributions).} 

\paragraph{YCSB-A (50\% put, 50\% get, Zipf and Zipf-prefix distributions).} 

\paragraph{YCSB-B (5\% put, 95\% get, Zipf and Zipf-prefix distributions).}

\paragraph{YCSB-C (100\% get, Zipf and Zipf-prefix distributions).}  

\paragraph{YCSB-D (5\% put, 95\% get, Latest distribution).}

\paragraph{YCSB-E (5\% put, 95\% scan, Zipf and Zipf-prefix distributions).}

\remove{
To drive the benchmarks, we use the 
YCSB framework in C++\footnote{\url{https://github.com/basicthinker/YCSB-C}}  

All three systems are configured to provided persistence, namely, no data loss, by using their synchronous logging mode.
In addition, we deploy a single partition in each of the systems in order to provide consistent scans across the entire data store.

We do not compare with Tucana~\cite{tucana} as its code has not been available; moreover, it does not support the persistency and consistency guarantees that \sys\ provides. 
}