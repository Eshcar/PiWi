We extensively compare the \sys\/ prototype to RocksDB -- a mature industry-leading KV-store implementation  -- under a variety of scenarios. \remove{RocksDB is used as storage layer of multiple popular SQL and NoSQL databases, e.g., MyRocks~\cite{MyRocks} (incarnation of MySQL) and MongoRocks~\cite{MongoRocks} (incarnation of MongoDB). RocksDB is an LSM-tree that is highly optimized for both read and write scenarios. For example, its compaction scheduling policies are highly tuned to minimize impact on mainstream data access.} We use the most recent RocksDB release 5.17.2, available Oct 24, 2018.  
It is worth noting that RocksDB's performance improved extensively during the last two years, primarily through 
optimized LSM compaction scheduling~\cite{RocksDBProgress}.   

\subsection{Setup}
We employ a C++ implementation~\cite{Cpp-YCSB} of a widely popular YCSB~\cite{YCSB} benchmarking platform. 
YCSB provides a framework for KV-store comparisons, through a set of common data access API's and standard workloads. 
Most modern KV-stores implement  YCSB adapter API's. The platform decouples data access from workload generation, 
thereby providing common ground for backend comparison. A workload is characterized by a combination of get, put, 
and scan accesses, as well as by a synthetic distribution of keys and values. YCSB provides a set of benchmark workloads
inspired by real-life applications, and allows developing new ones through workload generation API's. 

A typical YCSB instance stress-tests the backend KV-store through a pool of concurrent worker threads that drive identical
workloads. It aggregates the key performance metrics for the scenario under test, e.g., total throughput and tail access latencies. 

\paragraph{Environment.}
Our experiment testbed is a 12-core Intel Xeon 5 machine with 4TB SSD disk. The driver application exercises 12 threads (maximum parallelism). In order to guarantee a fair memory allocation to all KV-stores, we run each experiment 
within a Linux container with 16 GB RAM. 

\paragraph{Data.} We scale the dataset size from 4 GB to 64 GB, in order to exercise multiple locality 
scenarios with respect to the available RAM. Similarly to the published RocksDB benchmarks, we use 
10-byte keys that YCSB pads with a fixed 4-byte prefix (effectively, 14-byte keys), and 800-byte values. 
The data is stored uncompressed. 

The dataset build stage sequentially fills an empty KV-store with the target number of KV-pairs. Following this, 
the evaluation stage performs 80 million data accesses, evenly spread across the worker threads.   

\paragraph{Workloads.} 

Our experiments leverage multiple workloads: 

\begin{enumerate}
\item {\em Zipf}. . The key frequencies are sampled from the heavy-tailed Zipf distribution, 
following the description in~\cite{?}, with $\theta = 0.8$. The keys themselves are sampled 
uniformly at random from the whole data range. Zipf exhibits medium locality -- e.g., the most 
popular key's frequency is approximately $0.7\%$. This is a standard YCSB workload that captures
a multitude of use cases -- e.g., a web page cache distribution. 

\item {\em Zipf-range}. Similar to Zipf ($\theta=0.8$), except that we sample the key's $14$ most significant bits
(MSBs) rather than the complete key. The remainder of the key sampled uniformly at random. Zipf-range exhibits
higher locality than Zipf. This workload captures a multitude of mini-clusters of varying popularity -- 
for example, user posts in a social network~\cite{linkbench} or mobile app event logs~\cite{flurry}. 
In this setting, the key is structured as a combination of the user (resp., app) id and the post (resp., event) id. 

\item {\em Latest}. New records are inserted, and the most recently inserted records are the 
most popular (e.g., status updates and reads). This is a standard YCSB workload with very high 
locality. 
\end{enumerate}

The workloads exercise different mixes of puts, gets, and scans. We use standard YCSB scenarios 
(A to E) that capture write-savvy ($50\%$ puts) to read-savvy ($95\%-100\%$ gets or scans) settings. 

In order to stress the system even more on the write side, we introduce a new workload, named 
YCSB-P, comprised of $100\%$ puts. It captures a heavy-duty data load scenario (e.g., from an 
external data pipeline). 

\paragraph{Configuration.} 

We only present the performance results that exercise asynchronous puts -- as synchronous puts 
are approximately 10 times slower, thereby trivializing the performance results of every scenario 
they are part of. 

We set the \sys\/ hard chunk size limit to 10 MB, and the rebalance threshold to $0.7$ -- that is, 
most chunks are bound by 7 MB. 

We allocate a quota of 11 GB memory (out of the 16 GB container) to the munk cache and the row cache. 
For the Zipf and Latest workloads, 7 GB are given to munks. For the Zipf-range workloads, 9 GB 
are given to munks.  We limit the write buffer size to 2 MB for write-intensive workloads, and to $0.5$ MB 
for read intensive-workloads. 
  
We tune all RocksDB runtime parameters in accordance with its system performance guide~\cite{RocksDBPerf}.   

\subsection{Results -- Mainstream Scenarios}

\begin{figure*}[tb]
\centering
\begin{subfigure}{0.3\linewidth}
\caption{YCSB-P}
\label{fig:throughput:p}
\end{subfigure}
\hspace{0.2\linewidth}
\begin{subfigure}{0.3\linewidth}
\caption{YCSB-A}
\label{fig:throughput:a}
\end{subfigure}
\begin{subfigure}{0.3\linewidth}
\caption{YCSB-B}
\label{fig:throughput:b}
\end{subfigure}
\begin{subfigure}{0.3\linewidth}
\caption{YCSB-C}
\label{fig:throughput:c}
\end{subfigure}
\begin{subfigure}{0.3\linewidth}
\caption{YCSB-D}
\label{fig:throughput:d}
\end{subfigure}
\begin{subfigure}{0.3\linewidth}
\caption{YCSB-E10}
\label{fig:throughput:e10}
\end{subfigure}
\begin{subfigure}{0.3\linewidth}
\caption{YCSB-E100}
\label{fig:throughput:e100}
\end{subfigure}
\begin{subfigure}{0.3\linewidth}
\caption{YCSB-E1000}
\label{fig:throughput:e1000}
\end{subfigure}
\caption{\bf{\sys\/ versus RocksDB throughput, under multiple workloads and scaling dataset sizes.}}
\label{fig:throughput}
\end{figure*}

\begin{figure*}
\centering
\hspace{0.05\linewidth}
\begin{subfigure}{0.3\linewidth}
\caption{YCSB-P}
\label{fig:writeamp:p}
\end{subfigure}
\hspace{0.05\linewidth}
\begin{subfigure}{0.25\linewidth}
\caption{YCSB-A}
\label{fig:writeamp:a}
\end{subfigure}
\caption{\bf{\sys\/ versus RocksDB write amplification, under write-intensive workloads and scaling dataset sizes.}}
\label{fig:writeamp}
\end{figure*}

%\subsubsection{Throughput}
Figure~\ref{fig:throughput} summarizes the overall system throughput (operations per second) 
measured across all scenarios and dataset sizes. 
%Figure~\ref{fig:writeamp} presents write amplification under write-intensive workloads 
%(YCSB-P and YCSB-A). 

\paragraph{YCSB-P (100\% put, Zipf and Zipf-range distributions).} 
\sys's throughput is $30\%$ to $50\%$ above RocksDB's across the board 
(Figure~\ref{fig:throughput:p}). This is explained by \sys's significant reduction 
in disk I/O versus the LSM design in write-intensive scenarios, for all dataset sizes: 

\begin{enumerate}
\item {\em Small dataset (8GB or less).} \sys\/ accommodates munks for all chunks,
and manages all puts in memory. The munks are flushed to disk infrequently, in order 
to roll the chunk WAL's. In contrast, RocksDB performs on-disk compactions from time 
to time, due to its LSM-tree structure.   

\item{\em Big dataset (16 GB or more).} \sys\/ manages the popular chunks in RAM, 
and resorts to periodic on-disk funk rebalances for the less less popular chunks. The 
funk rebalance frequency depends on the chunk's popularity. In contrast, RocksDB 
compactions rewrite on-disk data regardless on whether it is hot or cold. As the dataset
becomes bigger, RocksDB spreads into more levels, hence its compactions become 
more frequent and involve more data.  
\end{enumerate}

Figure~\ref{fig:writeamp:p} corroborates the above observations, by comparing 
\sys's write amplification to RocksDB's. \sys\/ reduces the disk write rate dramatically, 
with the largest gain observed for big datasets (e.g., $1.15$ versus $3.03$ for the 32 GB 
database). As a nice by-product, \sys\/ is also more efficient with respect to disk wear.  

As expected, with small datasets \sys\/ performs better under the Zipf-range workload  
in comparison with the Zipf workload, due to higher locality. 

\paragraph{YCSB-A (50\% put, 50\% get, Zipf and Zipf-range distributions).}

This scenario is particularly challenging because it stresses both the write and the read paths. 
The latter exercises \sys's munk cache as well as its row cache (which was not used by the previous 
scenario). Table~\ref{tab:hitrate} summarizes the hit rate by cache type, to explain the read performance. 

Similarly to YCSB-P, we analyze two scenarios: 

\begin{enumerate}
\item {\em Small dataset.} Both \sys\/ and RocksDB exhibit near-perfect hit ratio. RocksDB needs to search 
both its memtable and block cache, whereas \sys\/ serves all puts and gets directly from munks. Overall, it
achieves $50\%$ to $100\%$ better throughput.

\item {\em Big dataset.} \sys's read performance depends on the workload's locality. For example, 
the Zipf-range workload is highly local, and is therefore served effectively by \sys's munk cache. 
Its throughput is $30\%$ to $50\%$ higher than RocksDB's across the board. The Zipf workload
is more dispersed, and therefore, \sys\/ resorts to the row cache and I/O optimizations to serve it.  
Thanks to its fine granularity, the row cache serves a big fraction of gets that are not covered by the munks. 
For example, for the 64 GB dataset it serves $74\%$ of the gets that miss the munk cache. The rest are served 
from the disk data structures -- the key store and the WAL. TBD ... 

\end{enumerate}

The write-path behavior is similar to YCSB-P (e.g., see Figure~\ref{fig:writeamp:a} for the write
amplification statistics). 

\paragraph{YCSB-B (5\% put, 95\% get, Zipf and Zipf-prefix distributions).}

\paragraph{YCSB-C (100\% get, Zipf and Zipf-range distributions).}  

\paragraph{YCSB-D (5\% put, 95\% get, Latest distribution).}

\paragraph{YCSB-E (5\% put, 95\% scan, Zipf and Zipf-range distributions).}
We experiment with short-to-medium scans - 10, 100, and 1000 rows. 

\subsection{Results -- Recovery Scenarios}

\remove{
To drive the benchmarks, we use the 
YCSB framework in C++\footnote{\url{https://github.com/basicthinker/YCSB-C}}  

All three systems are configured to provided persistence, namely, no data loss, by using their synchronous logging mode.
In addition, we deploy a single partition in each of the systems in order to provide consistent scans across the entire data store.

We do not compare with Tucana~\cite{tucana} as its code has not been available; moreover, it does not support the persistency and consistency guarantees that \sys\ provides. 
}
