We implement \sys\ in C++. We borrow the \code{SSTable} implementation from the RocksDB open source~\cite{RocksDB}.
Similarly to RocksDB, we use \code{jemalloc} for memory allocation.  

The following provides details about \sys's algorithmic aspects that have not been addressed in~\cref{sec:design}.

%PO array is split into PSA and per-chunk PPAs, as kiwi; DO NOT MENTION employs  helping as in KiWi. 
\paragraph{Chunk index.}  Implemented as an array sorted by the bottom boundary key. 
Whenever a new chunk is created (upon split), the index is rebuilt and the reference to it is atomically flipped
(copy-on-write). We found this simple implementation to be fastest for infrequently changing metadata. 

\paragraph{Synchronization data structures.}
%We note that 
Using a single pending array to synchronize all
operations can cause unnecessary contention.
We mitigate this problem in our implementation by maintaining two data structures for coordinating different operations (similar to~\cite{kiwi}). The first is a per-chunk pending put array (\emph{PPA}) which 
either indicates the current update's key and version, or indicates that the thread is currently not performing a put.
%maps each thread either to a cell in the chunk that the thread is currently attempting to put into and a corresponding version, or an indication that the thread is currently not performing a put. 
The second is a global pending scan array (\emph{PSA}) which tracks versions used by pending scans for compaction purposes; each entry consists of a version \code{ver} and a sequence number \code{seq}, as well as the scanâ€™s key range. Each entry in the \code{PPA}s and \code{PSA} includes, in addition to the operation metadata, an ABA sequence number. 

A put operation consists of 5 phases: (1) \emph{pre-process} - locate the target chunk; if a munk exists, prepare a cell to insert the value into; (2) \emph{publish} - obtain a version while synchronizing with concurrent scans and rebalances via the chunk's lock and publish indication in the chunk's \code{PPA}; (3) \emph{persist} - write the data into the log, indicate it is persisted in the \code{PPA}; (4) \emph{link} - if a munk exists, connect the new cell to the linked list, so it can be found through the list traversal, otherwise, update the row cache to latest version if key is present in the cache; and finally (5) \emph{clean} - clear the entry in the chunk's \code{PPA}, and increase the entry's ABA number.
If the put fails to acquire the chunk's lock (since it is being rebalanced), the operation restarts, and {\inred{re-attempts to}} find an active chunk.
Puts trigger both munk and funk rebalances. The former are handled inline when the munk {\inred{is close to overflow}}; the latter are done in the background 
by helper threads.

%Thus the version number is composed of the GV and 
A per-chunk linearization counter is used to determine the order of concurrent put operations updating the same key.  
Note that multiple \emph{generations} of munks may exist for a chunk throughout its life time.
Thersfore the linearization counter is composed of three parts: (a) the GV value; (b) a \emph{generation number} - incremented whenever a munk is cached into memory and when the munk is rebalanced; and (c) a \emph{sequence number} - incremented upon each put and set to the number of KV-pairs in a munk upon a new generation number. When a new chunk is created as a result of a split, the children chunks inherit their generation number from their parent. The linearization counter is written both to the \code{PPA} upon publishing the put operation, and to the \code{log} when persisting the data. This ensures all operations see the same order of writes per key.

A scan operation first publishes its intent to obtain a version in the \code{PSA}. It determines its scan time $t$ by increasing GV and writing it to its entry in the \code{PSA}. The scan operation then starts traversing the chunks in its range. For each chunck, first wait for all put operations that are either with smaller version than $t$ or still have not acquired a version to clear their entry in the \code{PPA} or acquire a larger version. After waiting for all concurrent puts to complete, the scan operation can read the range from the chunk. If a munk exists simply read the range from the linked list, skipping versions that are not last before $t$. Otherwise, the scan merges the \code{SSTable} and \code{log} data and reads the range from the result, again skipping the penultimate versions. When the scan completes, it clears the entry in the \code{PSA}, and increases the entry's ABA number. Get operations access neither the \code{PSA} nor the \code{PPA}.% data structures.

A munk rebalance iterates through the \code{PSA} to collect the maximum version number among the active scans that cannot be reclaimed yet. If a scan published its intent in the \code{PSA} but published no version number yet, the rebalance waits until either the version is published or the ABA number in the entry is increased. 
\remove{It then acquires the chunk's lock to block additional put operation. If executing a funk rebalance it also acquires the funk rebalance lock for the chunk. After completeing the rebalance operation and placing the new content of the chunk in place including the updated metadata, all locks are promoted.
}

%\paragraph{Adaptive mechanisms.} \sys\ applies two dynamic mechanisms that adapt to the workload. 
\paragraph{Munk caching policy.}
%The first is the \emph{munk caching policy}, which is crucial for locality of access. 
We apply simple LFU caching,
in which the score is a weighted average of the number of accesses per operation type. We 
use exponential decay to maintain the recent access counts, similar to~\cite{tinyLFU}: periodically, all counters are 
sliced by a factor of two. 

%where the frequency is actually a score that gives a different weights to each operation type.
%We employ a sliding window scoring mechanism, that scores each chunks only based on recent accesses. The sliding window is implemented by occasionally slicing %the counters of all chunks by a factor of two, as was done in~\cite{tinyLFU}.

%Funk rebalance frequency is also crucial for the performance of the system. Excessive rebalances may reduce ingestion throughput, while immensely reducing funk rebalance frequency can degrade the performance of scans that are required to sequencially traverse a very long log. To this end, \sys\ exploits the per-operation type counters also to determine the frequency in which each funk is rebalanced.

\paragraph{Row cache.} 
We implement a coarse-grained LRU policy by maintaining a fixed-size list of hash tables. Only the first table is written to. Once the number of keys in the table reaches a threshold, an empty table is placed in the front of the list, and the last table in the list is discarded. Consequently, lookups for recently cached keys are usually served by the first table, and unpopular keys are removed from the cache once their table is discarded.
The row cache must never serve stale values. %, yet need not contain each and every updated key. 
Therefore, a put updates the cache  if a previous version of that key is already in the cache. 
If the key is not present in the row cache the put operation does not update the cache, to avoid over populating the cache in write dominated workloads. 
After a get operation, the up-to-date key is always in the first table - either because it was already there; it was found elsewhere and just inserted; or it was on an inner table and moved forward to the first one. The latter ensures the key will remain longer in the cache. While the key's entry is duplicated, its value is shared by the two entries, eliminating a much larger duplication.

\remove{We implement an LRU policy for the row cache. It is composed of 3 hash tables: 2 are static (read-only), and the most recent is dynamic. If a read operation does not find a key in the row cache it fetches its value from the STable and stores it in the dynamic table. If the value is present in one of the static tables but not in the dynamic one, it is moved forward to the dynamic table to indicate it is a hot key. A put operation also updates the row cache in case the key is present. We use per-bucket locks to make sure the latest version is updated in the row cache. If the key is not present in the row cache the put operation does not update the cache, to avoid over populating the cache in write dominated workloads. Upon filling the dynamic table (its size exceeds a threshold), the first table is discarded, the dynamic table becomes static and a new empty dynamic table is created.
}

\paragraph{Chunk merges support.}
%\inred{move to conclusion and future work?}
Our current implementation does not support chunk merges, to defragment the store after massive data deletion. This could be done as 
part of the rebalance procedure (see~\cite{kiwi}).
\remove{In \sys\ this entails merging the funks of two chunks. As in the split operation, the rebalance first acquires the  rebalance locks of the chunks to be merged---to ensure exclusiveness. The content of the chunks is merged and written into a new chunk. Finally, the rebalance acquires the chunks' locks for the short period in which the content that was added to the chunks during the merge is written to the log of the new chunk, and the new chunk swaps the old chunks in the list.
}