We implement \sys\ in C++. It is partially based on RocksDB open source library~\cite{rocksdb}.
Specifically, the implementation of \sys\ STables is based on the SSTable code in RocksDB since it supports similar functionality of append writes and efficient searches. 

%PO array is split into PSA and per-chunk PPAs, as kiwi; DO NOT MENTION employs  helping as in KiWi. 
\paragraph{Synchronization Data Structures}
We note that using a single pending array to synchronize all
operations can cause unnecessary contention.
We mitigate this problem in our implementation by maintaining two data structures for coordinating different operations similar to the solution used in~\cite{kiwi}. The first is a per-chunk pending put array (PPA) maps each thread either to a cell in the chunk that the thread is currently attempting to put into and a corresponding version, or an indication that the thread is currently not performing a put. The second is a global pending scan array (PSA) tracks versions used by pending scans for compaction purposes; each entry consists of a version ver and a sequence number seq, as well as the scanâ€™s key range. Each entry in the PPAs and PSA includes, in addition to the operation metadata, an aba~\footnote{ABA phenomena- ...} sequence number. 

A put operation consists of 5 phases: (1) \emph{pre-processing} - locate the target chunk and if a munk exists prepare a cell to insert with the written value; (2) \emph{publishing} - obtain a version while synchronizing with concurrent scans and rebalances via the chunk's lock and publish its existence in the chunk's PPA; (3) \emph{persisting} - write the data into the log, indicate it is persisted in the PPA; (4) \emph{linking} - if a munk exists, connect the new cell to the linked list, so it can be found through the list traversal, otherwise, update cache to latest version if key is present in the cache; and finally (5) \emph{cleaning} - clear the entry in the chunk's PPA, and increase the entry's aba number.
If the put operation is unable to acquire the chunk's lock (since it is being rebalanced) the operation restarts, hoping it will find an active chunk this time.

The version number is composed of the GV and a per-chunk linearization number that is used to determine the order of put operations to the same key with the same GV value.  The sequence number is composed of two parts: (a) \emph{a generation number} - incremented whenever a munk is cached into memory and when the munk is rebalanced; and (b) \emph{a sequential number} - incremented upon each put operation and is set to the number of cells in a munk upon a new generation number. When a new chunk is created as a result of a split, the child chunks inherit their generation number from their parent. The version number is written both to the PPA upon publishing the put operation, and to the log when persisting the data. This ensures all operations see the same order of writes per key.

The scan operation first publishes its intent to obtain a version in the PSA. It determines its scan time $t$ by increasing GV and writing it to its entry in the PSA. The scan operation then starts traversing the chunks in its range. For each chunck, first wait for all put operations that are either with smaller version than $t$ or still have not acquired a version to clear their entry in the PPA or acquire a larger version. After waiting for all concurrent puts to complete, the scan operation can read the range from the chunck. If a munk exists simply read the range from the linked list, skipping versions of keys that are not last before $t$. Otherwise, the scan merge-sort the content of the STable and the log and read the range from the result, again skipping versions of keys that are not last before $t$. When the scan completes it clears the entry in the PSA, and increase the entry's aba number. Get operations do not access neither the PSA nor the PPA data structures.

Finally, rebalance operations go through the PSA to collect the maximum version number of active scans that can not be reclaimed yet. If a scan publishes an intent in the PSA but not yet a version number the rebalance operation waits until a version is published or the aba number in the entry is increased. It then acquires the chunk's lock to block additional put operation. If executing a funk rebalance it also acquires the funk rebalance lock for the chunk. After completeing the rebalance operation and placing the new content of the chunk in place including the updated metadata, all locks are released.

\paragraph{Dynamic Mechanisms}
\sys\ has two dynamic mechanisms that adjust to the running workload. The \emph{chunks cache policy} is one of the core components of the system as most benefits of \sys\ are due to storing the working set in-memory. We apply a simple caching policy. 
%It caches hot chunks  in-memory while evicting least-recentle-used (LRU) chunks off the the memory. 
The chunks are scored based on their access frequency, using per access type counters. We employ a sliding window scoring mechanism, that scores each chunks only based on recent accesses. The sliding window is implemented by occasionally slicing the counters of all chunk by a factor of two, as was done in~\cite{tinyLFU}.

Funk rebalance frequency is also crucial for the performance of the system. Excessive rebalances may reduce ingestion throughput, while immensely reducing funk rebalance frequency can degrade the performance of scans that are required to sequencially traverse a very long log. To this end, \sys\ exploits the per-access type counters also to determine the frequency in which each funk is rebalanced.

\paragraph{Row Cache} We implement an LRU policy for the row cache. It is composed of 3 hash tables: 2 are static (read-only), and the most recent is dynamic. If a read operation does not find a key in the row cache it fetches its value from the STable and stores it in the dynamic table. If the value is present in one of the static tables but not in the dynamic one, it is moved forward to the dynamic table to indicate it is a hot key. A put operation also updates the row cache in case the key is present. We use per-bucket locks to make sure the latest version is updated in the row cache. If the key is not present in the row cache the put operation does not update the cache, to avoid over populating the cache in write dominated workloads. Upon filling the dynamic table (its size exceeds a threshold), the first table is discarded, the dynamic table becomes static and a new empty dynamic table is created.

\paragraph{Supporting Merges}
\inred{move to conclusion and future work?}
Our current implementation does not support chunk merges. Merge operation can be supported as part of the rebalncing procedure (as was done in~\cite{kiwi}). In \sys\ this entails merging the funks of two chunks. As in the split operation, the rebalance first acquires the  rebalance locks of the chunks to be merged---to ensure exclusiveness. The content of the chunks is merged and written into a new chunk. Finally, the rebalance acquires the chunks' locks for the short period in which the content that was added to the chunks during the merge is written to the log of the new chunk, and the new chunk swaps the old chunks in the list.
