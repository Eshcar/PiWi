We implement \sys\ in C++. It is partially based on RocksDB open source library~\cite{rocksdb}.
Specifically, \code{SSTable} is based on the SSTable code in RocksDB as it supports similar functionality of append writes and efficient searches. 

%PO array is split into PSA and per-chunk PPAs, as kiwi; DO NOT MENTION employs  helping as in KiWi. 
\paragraph{Synchronization Data Structures}
We note that using a single pending array to synchronize all
operations can cause unnecessary contention.
We mitigate this problem in our implementation by maintaining two data structures for coordinating different operations (similar to~\cite{kiwi}). The first is a per-chunk pending put array (\emph{PPA}) which 
either indicates the current update's key and version, or indicates that the thread is currently not performing a put.
%maps each thread either to a cell in the chunk that the thread is currently attempting to put into and a corresponding version, or an indication that the thread is currently not performing a put. 
The second is a global pending scan array (\emph{PSA}) which tracks versions used by pending scans for compaction purposes; each entry consists of a version \code{ver} and a sequence number \code{seq}, as well as the scanâ€™s key range. Each entry in the \code{PPA}s and \code{PSA} includes, in addition to the operation metadata, an ABA sequence number. 

A put operation consists of 5 phases: (1) \emph{pre-processing} - locate the target chunk and if a munk exists prepare a cell to insert with the written value; (2) \emph{publishing} - obtain a version while synchronizing with concurrent scans and rebalances via the chunk's lock and publish indication in the chunk's \code{PPA}; (3) \emph{persisting} - write the data into the log, indicate it is persisted in the \code{PPA}; (4) \emph{linking} - if a munk exists, connect the new cell to the linked list, so it can be found through the list traversal, otherwise, update cache to latest version if key is present in the cache; and finally (5) \emph{cleaning} - clear the entry in the chunk's \code{PPA}, and increase the entry's ABA number.
If the put operation is unable to acquire the chunk's lock (since it is being rebalanced) the operation restarts, hoping it will find an active chunk this time.
Puts invoke both munk and funk rebalances. The former is handled immediately when the munkspace is exhausted, while the latter is done in the background by some helper thread.

%Thus the version number is composed of the GV and 
A per-chunk linearization counter is used to determine the order of concurrent put operations updating the same key.  
Note that multiple \emph{generations} of munks may exist for a chunk throughout its life time.
Thersfore the linearization counter is composed of three parts: (a) the GV value; (b) \emph{a generation number} - incremented whenever a munk is cached into memory and when the munk is rebalanced; and (c) \emph{a sequence number} - incremented upon each put operation and set to the number of cells in a munk upon a new generation number. When a new chunk is created as a result of a split, the children chunks inherit their generation number from their parent. The linearization counter is written both to the \code{PPA} upon publishing the put operation, and to the \code{log} when persisting the data. This ensures all operations see the same order of writes per key.

The scan operation first publishes its intent to obtain a version in the \code{PSA}. It determines its scan time $t$ by increasing GV and writing it to its entry in the \code{PSA}. The scan operation then starts traversing the chunks in its range. For each chunck, first wait for all put operations that are either with smaller version than $t$ or still have not acquired a version to clear their entry in the \code{PPA} or acquire a larger version. After waiting for all concurrent puts to complete, the scan operation can read the range from the chunck. If a munk exists simply read the range from the linked list, skipping versions of keys that are not last before $t$. Otherwise, the scan merge-sort the content of the \code{SSTable} and the \code{log} and read the range from the result, again skipping versions of keys that are not last before $t$. When the scan completes it clears the entry in the \code{PSA}, and increase the entry's ABA number. Get operations neither access the \code{PSA} nor the \code{PPA} data structures.

Finally, rebalance operations go through the \code{PSA} to collect the maximum version number of active scans that can not be reclaimed yet. If a scan publishes an intent in the \code{PSA} but not yet a version number the rebalance operation waits until a version is published or the ABA number in the entry is increased. 
\remove{It then acquires the chunk's lock to block additional put operation. If executing a funk rebalance it also acquires the funk rebalance lock for the chunk. After completeing the rebalance operation and placing the new content of the chunk in place including the updated metadata, all locks are released.
}

\paragraph{Dynamic Mechanisms}
\sys\ has two dynamic mechanisms that adjust to the running workload. The \emph{chunks cache policy} is one of the core components of the system as most benefits of \sys\ are due to storing the working set in-memory. We apply a simple LFU caching policy,
where the frequency is actually a score that gives a different weights to each operation type.
We employ a sliding window scoring mechanism, that scores each chunks only based on recent accesses. The sliding window is implemented by occasionally slicing the counters of all chunks by a factor of two, as was done in~\cite{tinyLFU}.

Funk rebalance frequency is also crucial for the performance of the system. Excessive rebalances may reduce ingestion throughput, while immensely reducing funk rebalance frequency can degrade the performance of scans that are required to sequencially traverse a very long log. To this end, \sys\ exploits the per-operation type counters also to determine the frequency in which each funk is rebalanced.

\paragraph{Row Cache} 
We implement a coarse-grained LRU policy by maintaining a fixed-size list of hash tables. Only the first table is written to. Once the number of keys in the table reaches a threshold, an empty table is placed in the front of the list, and the last table in the list is discarded. Consequently, lookups for recently cached keys are usually served by the first table, and unpopular keys are removed from the cache once their table is discarded.
The row cache must never serve stale values. %, yet need not contain each and every updated key. 
Therefore, a put updates the cache  if a previous version of that key is already in the cache. 
If the key is not present in the row cache the put operation does not update the cache, to avoid over populating the cache in write dominated workloads. 
After a get operation, the up-to-date key is always in the first table - either because it was already there; it was found elsewhere and just inserted; or it was on an inner table and moved forward to the first one. The latter ensures the key will remain longer in the cache. While the key's entry is duplicated, its value is shared by the two entries, eliminating a much larger duplication.

\remove{We implement an LRU policy for the row cache. It is composed of 3 hash tables: 2 are static (read-only), and the most recent is dynamic. If a read operation does not find a key in the row cache it fetches its value from the STable and stores it in the dynamic table. If the value is present in one of the static tables but not in the dynamic one, it is moved forward to the dynamic table to indicate it is a hot key. A put operation also updates the row cache in case the key is present. We use per-bucket locks to make sure the latest version is updated in the row cache. If the key is not present in the row cache the put operation does not update the cache, to avoid over populating the cache in write dominated workloads. Upon filling the dynamic table (its size exceeds a threshold), the first table is discarded, the dynamic table becomes static and a new empty dynamic table is created.
}

\paragraph{Supporting Merges}
%\inred{move to conclusion and future work?}
Our current implementation does not support chunk merges. Merge operation can be supported as part of the rebalncing procedure (as was done in~\cite{kiwi}). 
\remove{In \sys\ this entails merging the funks of two chunks. As in the split operation, the rebalance first acquires the  rebalance locks of the chunks to be merged---to ensure exclusiveness. The content of the chunks is merged and written into a new chunk. Finally, the rebalance acquires the chunks' locks for the short period in which the content that was added to the chunks during the merge is written to the log of the new chunk, and the new chunk swaps the old chunks in the list.
}