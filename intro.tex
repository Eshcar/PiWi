

\remove{
% Removed RT analytics - not sure this is our pitch
Real-time analytics applications are on the rise in the big data world. 
Modern decision support and machine intelligence engines strive to continuously ingest large volumes of data, while providing up-to-date insights with minimum delay. 
Examples of such systems include Google's F1~\cite{Shute2013}, which powers its AdWords, and 
Oath's Flurry Analytics\footnote{\url{https://developer.yahoo.com/analytics/}}, which 
provides mobile developers with analysis tools to explore user characteristics (age, gender, location, app context, etc.) and behavior, e.g., which code paths they follow and how they churn.
As of late 2017, the Flurry SDK is installed on 2.6B devices, and monitors 1M+ mobile apps\footnote{\url{http://flurrymobile.tumblr.com/post/169545749110/state-of-mobile-2017-mobile-stagnates}}. 
Analytics workloads feature a combination of high ingestion rates and (concurrently executed) large range scans; their  
data -- both ingested and queried -- is highly skewed towards popular applications, locations, users, etc. 
Moreover, since applications like Flurry often aggregate statistics for multi-dimension composite keys (e.g., application-user-date)  
and scan them in ranges, the skewness is reflected for key ranges rather than individual keys. 

Underlying today's real-time analytics systems are large \emph{key-value (KV)-stores}, such as RocksDB~\cite{rocks},  
to which data is ingested at a high rate, and which simultaneously serve 
\emph{range scans} for analytics queries. Such data stores need to be \emph{persistent} following crashes.   
}

% Motivation
The booming NoSQL market, which broadly aggregates the technologies for storage, serving and analysis of very large volumes of unstructured data, is expected to garner \$4.2B by 2020~\cite{alliedmarketresearch}. %\footnote{\url{https://www.alliedmarketresearch.com/press-release/NoSQL-market-is-expected-to-reach-4-2-billion-globally-by-2020-allied-market-research.html}}. 
Key-value (KV)-stores~\cite{Aerospike, Bigtable, Cassandra, DynamoDB, HBase, Redis, RocksDB, Scylla} are the fastest-growing market segment among the NoSQL platforms. They offer a simple yet powerful data abstraction that is  attractive for a variety of applications, e.g., e-commerce~\cite{DynamoDB}, search indexing~\cite{Percolator}, product recommendation~\cite{NetflixCassandraSpark}, user modeling~\cite{AirbnbHBase}, etc. Modern KV-storage technologies provide scalability and speed far beyond the reach of traditional RDBMS technologies, at a fraction of cost. The new generation of relational databases~\cite{MyRocks, Phoenix} uses KV-store infrastructure as building block, in order to apply its multiple benefits to structured data management. 

% Analytics, DRAM, persistence
The drop in DRAM prices (more than $6.6$x since 2010~\cite{StatistaReport}) stimulates proliferation of 
KV-stores towards better use of memory. One extreme approach has been taken by in-memory data stores
that store all their data in RAM. While appealing to many use cases (e.g., web caching~Ö¿\cite{RedisWebCaching}, online fraud detection~\cite{Hazelcast}, and real-time bidding~\cite{AerospikeUseCRaases}), they fall short when it comes to scalability and reliability requirements. Recently, some of these products~\cite{Aerospike,Ignite,Redis,Tarantul} started supporting non-volatile
(primarily, solid state) disk drives for both cold data storage and crash recovery. They strive to provide the user experience 
of in-memory databases (ultra low latency, flexibility for diverse workloads) while serving very large datasets with near-zero 
time to recovery. Our paper steps up to the challenge of building such {\em memory-centric\/} KV-stores. 

\remove{
One domain that is on the rise is real-time analytics, and it uses big data and requires long scans [support  by examples/citations]. 
High performance is achieved by striving to keep almost all of the working data set in-memory, which is becoming increasingly feasible 
with the drop in DRAM prices [citations].  
Nevertheless, disks are still required for data persistence. 
Note that even in cache-only KV-stores that rely on a separate cold storage tier for persistent archiving, 
using a RAM-only solution for real-time query processing is undesirable, as the the penalty for crashes is extremely high; for example,  
Facebook reported that it would take days to recover a crashed cluster from the backend persistent store, and this is expedited to take hours  
by recovering from so-called warm clusters that hold the data in memory~\cite{Scaling-Memcache}. 
%(try to find a citation - maybe something Facebook wrote about this). 
This motivates optimizing key-value stores for workloads where all or most data fits in memory, while supporting persistence with fast recovery. 
}

% KV stores today are LSM
KV-stores provide an {\em ordered map\/} API, with random read (get), random write (put), and range query 
(scan) methods. By and large, their implementations are based on the \emph{Log-Structured Merge (LSM)} 
tree paradigm~\cite{LSM}, which optimizes write performance by replacing random writes with sequential ones. 
An LSM tree is organized as a collection of components, partitioned according to data update times. 
The most recently written data resides in the in-memory map ({\em memtable\/}), and earlier-written data 
resides in a collection of sorted immutable on-disk ({\em level}) files. Every write creates a  data version, 
which is inserted into the memtable, and in addition, for persistence, is logged to a \emph{write-ahead-log} ({\em WAL}). Periodically, the whole memtable is flushed to disk, creating a new level file. Reads search the memtable first,
and if the key is not found, also the level files; the latest version is retrieved. To expedite reads, popular data blocks 
from level files are cached in memory, and Bloom filters reduce redundant accesses to disk. Scans traverse the 
memtable and the level files in parallel, and combine the data versions in the ascending/descending order of keys. 
In order to reduce disk reads, level files are periodically merged by a background process named \emph{compaction}.
 
% LSM  drawbacks
Despite its popularity, the LSM paradigm suffers from a number of drawbacks. 
First, it is subject to \emph{write amplification} -- namely, each key-value pair is written multiple times. 
WAL logging implies that updates are written at least twice, and may be written many more times due 
to compactions. Write amplification increases disk wear (especially for SSD storage), in addition to hurting 
performance. Second, LSMs incur \emph{read amplification} -- namely, reads may need 
to search in multiple level files. This is partially mitigated by compaction, caching, and Bloom filters. 
All the above, however, are less effective for scans, which do not enjoy Bloom filters and suffer from 
caches being destroyed by compactions.  Though their impact can be reduced, both kinds of amplification 
are inherent to the LSM design, and cannot be avoided altogether. Additionally, recovery from crashes 
may be slow because the WAL needs to be replayed before the system proceeds to service new requests.  
Moreover, recovery time is adversely affected by available memory: increasing the memtable size 
reduces the flush rate, which slows down WAL recycling, and consequently prolongs recovery. 

% PiWi
%In this work we specifically target analytics workloads, with high ingestion rates and range scans of popular key ranges.
In this work we propose a novel paradigm for KV-storage, diverging from the ubiquitous LSM approach.  
We design and implement  \sys,  a \emph{persistent key-value store}, which minimizes read and write
amplification and expedites atomic range scans. 
%Whenever the data set entirely fits in memory, \sys\ writes data to a single memory location and a single disk location and reads data from a single memory location. 
 \sys\ further offers almost immediate recovery from  crashes. 
 
 \remove{
To minimize amplification, \sys\ leverages a common data structure for storing data in-memory and on-disk, 
encompassing the data store as well as the WAL and the disk cache, thus unifying the read and write paths. 
Our approach reduces compaction by selectively compacting only frequently-updated parts of the data structure, and not cold ones.
It further reduces write amplification and I/O bottlenecks by eliminating the need for a WAL.
Moreover, scans of popular key ranges are served from the in-memory data structure without accessing disk.
} 

% Data structure
\sys's organization differs from that of LSM  in a number of significant ways: 
\begin{enumerate}
\item Instead of temporally partitioning data into files according to update times as in LSM, 
\sys\ organizes data as  \emph{chunks} holding contiguous key ranges, both in memory and on disk. 
Thus, reads
only need to look for a key in one memory location and at most one file. Moreover, scans are highly efficient.
%, both in-memory and on disk. 

\remove{
\item Whereas LSM buffers writes in a memory component and separately caches blocks of data for expediting reads,
\sys\ unifies the read and write paths, using a common cache of popular chunks (key ranges) for both. 
The cached in-memory chunks are called \emph{munks}. 
}
%Thus, reads need to access disk only in case the appropriate key 
%range is not cached, and need to check at most one disk location. 

\item \sys\ uses a single on-disk data representation, which replaces both the WAL and the data store.  
%A put operation writes the key-value pair exactly once on disk and at most once in memory.
%This is achieved by (i) 
%called \emph{funk} (file chunk)
 %and does not keep a WAL separately from the  data store.
This reduces write amplification %as data needs to be written to disk exactly once, 
and also expedites recovery from crashes by eliminating the need to replay the WAL. 
% ; and (ii) a common cache for reads and writes. 
\item Most compactions occur in memory, further reducing write amplification. This again leverages the fact that chunks holding
popular key ranges are cached and can be compacted in-memory; files holding cold data can be compacted infrequently. 
 \end{enumerate}


We have implemented \sys\ in C++. We compare it to version \Idit{what?} of RocksDB~\cite{rocks}, perhaps the most popular KV store implementation today.  
% Results
\Idit{Todo: Write something about results.
Explain what we are better at and what we are worse at.} 




