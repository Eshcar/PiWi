


% KV-stores are everywhere
Key-value stores (KV-stores) are widely used nowadays by a broad range of applications, and are projected
to continue to increase in popularity in years to come; market research  identifies them as the 
``driving factors'' of the NoSQL market, which is expected to garner \$4.2B by 2020~\cite{alliedmarketresearch}.

% In many applications, keys are composite
For performance reasons,  KV-stores  usually  employ a \emph{composite} primary key that represents
an agglomerate of attributes. Range scans are then used to collect all the rows associated with
a particular attribute. For instance,  in messaging and email applications,  
keys are typically a concatenation of user-id with additional fields such as thread-id, time, and post-id; 
example queries in Facebook Messages retrieve  the last 100 messages of a given user, or the hourly impression counts %over the last 24 hours 
for a given advertiser~\cite{Borthakur:2011:AHG:1989323.1989438}. 
Similarly, analytics applications such as Flurry~\cite{flurry}  aggregate statistics on data identified by 
composite keys consisting of attributes such as application-id, user-id, time, location, and so on~\cite{flurry-data-model}. 
Another example is social networks, which store  associations representing graph edges indexed by a primary key consisting of a 
pair of object-ids and an association type~\cite{linkbench}. 

% Composite keys imply spatial data locality (in ranges) 
Composite keys induce \emph{spatial locality} in workloads with high {temporal locality}, as 
popular entities (for example, users) result in popular \emph{key ranges}. 
Spatial locality may also arise with simple (non-composite) keys, for example in 
 reverse  URLs, which  are often used as keys for web search applications. 
While the prevalence of temporal locality (i.e., frequent access to popular entities) in real-world workloads is widely-recognized, 
and indeed standard benchmarks (e.g., YCSB~\cite{YCSB})  feature skewed key-access distributions such as Zipf, 
these benchmarks fail to capture the spatial aspect of locality.
This, in turn, leads to storage systems being optimized for a skewed distribution on individual keys with no spatial locality.
In this paper, we make spatial locality a first class  consideration, which 
leads us to rethink the design principles underlying today's popular KV-stores.

% LSM is the standard but not ideal for this  because of  write path  temporal grouping leading to fragmentation and write amplificaiton
The de facto standard approach to building KV-stores today is \emph{LSM} -- log-structured merge trees~\cite{lsm}. 
The LSM approach optimizes write performance by absorbing random writes in memory and periodically flushing 
them as sequential files to disk. While  sequential disk access dramatically improves I/O throughput, it 
is important to notice that the LSM design initially groups writes  into files \emph{temporally}, and not by key-range. 
A background \emph{compaction} process later merge-sorts any number of files, grouping data by keys. 
This approach is not ideal for workloads with high spatial locality for two reasons. 
First, a popular key range will be fragmented across many files during long periods (between compactions). 
Second, the compaction process is costly both in terms of performance 
(as it consumes high disk bandwidth) and in terms of \emph{write amplification}, namely the number of physical writes 
associated with a single application write. The latter is significant particularly in SSD as it increases disk wear. 
The temporal grouping of data means that compaction is indiscriminate with respect to key popularity:  
Since new (lower level) files are always merged with old (higher level) ones, 
a ``cold'' key range that has not been accessed since the  beginning of time continues to be repeatedly re-located
by  compactions.  

% LSM is the standard but not ideal for this also in read path.
Because LSM's in-memory component consists only of recently-written keys, 
it does not contain keys that are frequently read without being modified. 
This causes \emph{read amplification}, where a read has to search for the 
requested key in multiple locations. 
In order to optimize the read-path, LSMs  use a cache of popular file blocks and 
Bloom filters that reduce unnecessary file access. 
But  a key range is typically dispersed over multiple cache blocks and Bloom filters. 
This memory organization does not leverage spatial locality, 
resulting in sub-optimal use of memory resources in case such locality is present.
Furthermore, it does not naturally lend itself to range scans,
which are common with composite keys.

% LSM is not the best when the entire working set fits in memory
Finally, we note that LSM's temporal file organization optimizes disk I/O but induces a penalty on in-memory operation. 
For example, all keys -- including popular ones -- are flushed to disk periodically, even though persistence is assured via a separate 
\emph{write-ahead-log (WAL)}. 
This increases write amplification and also makes the flushed keys unavailable for fast read from memory. 
Whenever the system leverages sufficient DRAM to hold almost the entire working set,  such flushes lead to inefficient use of memory resources. 
The drop in DRAM prices (more than $6.6$x since 2010~\cite{StatistaReport})  and 
significant performance benefit DRAM offers make this scenario increasingly common.  

% Drum roll 
We present \sys, a persistent KV-store based on a novel paradigm that diverges from the ubiquitous LSM approach.  
Like  LSMs,  we optimize I/O by absorbing updates in memory and performing bulk writes to disk. 
And yet unlike the LSM approach, we partition data according to key ranges and not temporally.
Data is  organized (both on disk and in memory) in large \emph{chunks} holding contiguous key ranges.
Popular chunks are cached in DRAM for the benefit of  both the write-path and the read-path.
Chunks reduce the fragmentation of key ranges, resulting in 
(1) better read and write performance for workloads with spatial locality,  and
(2) faster range scans. 
Moreover, since chunks  are compacted in memory, writes are 
flushed to disk less frequently than LSM writes (relying on a WAL for persistence)
yielding 
(3) reduced write amplification, and 
(4) better performance with memory-resident working sets.
 
We have implemented \sys\ in C++. We compare it to the recent release of RocksDB~\cite{rocks}, 
an industry-leading KV-store  based on LSM design. Our experiments, based on the popular 
YCSB benchmark suite~\cite{YCSB}, show that \sys\/ significantly outperforms  RocksDB, 
especially when spatial  locality is high. 
For example, with composite keys and a memory-resident working set, \sys\  speeds up scans 
by up to 3x, writes by up to 1.6x, and (single-key) reads by up to 2.2x.
With larger working sets than the available DRAM, \sys\ outperforms RocksDB by 50\%--100\% on reads  and 
scans, and 25\% on writes. 
 \Idit{Check numbers.}

We do not claim, however, that \sys\/ is better than a mature LSM-tree implementation
under all circumstances -- there are scenarios in which RocksDB performs better (e.g., under low locality). 
The primary goal of our work is to draw attention to the importance of spatial locality in 
today's workloads, and to propose a design alternative to LSM that is better suited for such locality. 
As with all new approaches, there is ample  room for 
future improvements. 
 
 % The rest of this paper is organized as follows ... 
 This paper proceeds as follows:
We present our design principles in~\cref{sec:principles} and  the \sys\ algorithm 
in~\cref{sec:design}. We then discuss implementation details in~\cref{sec:impl} and evaluate 
\sys\ in~\cref{sec:eval}.  Finally,~\cref{sec:related}   surveys related work and~\cref{sec:conclusions}
concludes. 


