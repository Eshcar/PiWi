

\sys\ is a persistent key-value store supporting atomic \emph{put, get}, and  \emph{range scan} (or scan) operations. 
Scans are atomic in the sense that all values returned by a single scan belong to a consistent snapshot reflecting
the state of the data store at a unique point in time.

\remove{
\sys\ is geared for high-throughput analytics applications, which (1) perform many range scans; 
(2) strive to accommodate the entire working set in RAM (for performance), and yet require  (3)
persistence with fast recovery. 

We now detail the key requirements from \sys\ and the design choices we made in order to meet them.

\subsection{Requirements}

The key requirements from \sys\ are  the following:
}

We now detail the key requirements from \sys:

\emph{Optimizing for spatial locality and atomic scans.}
 Many applications support multi-dimensional exploration of data, 
 which they realize in a kv-store using composite keys, which induce spatial locality.  
 \remove{For example, a typical  KV-map used by an application like Flurry Analytics~\cite{flurry} 
 summarizes mobile traffic statistics grouped by a combination of dimensions like  date,  user-id, and application-id.}
A query that retrieves data pertaining to a particular dimension thus needs to atomically 
retrieve the values pertaining to a range of keys. 
%To favor analytics queries, it is important to optimize such scans. 
 

\emph{High performance and low write amplification} with memory-resident working sets.
To sustain high throughput, key-value stores nowadays leverage increasing DRAM sizes where they can hold most of the 
active data set. We strive for maximum performance in this ``high performance'' case.  
 Minimizing disk writes is important not only for performance, but also for disk wear reduction.

\emph{Fast recovery to consistent state.}
Because crashes are inevitable, the mean-time-to-recover from crashes should be kept short.
Like other popular KV-stores~\cite{RocksDB,leveldb,hbase}, \sys\ supports \emph{asynchronous} persistence, 
where put requests are buffered in memory and periodically persisted to disk in bulk. 
This approach significantly reduces the latency of put operations, which do not wait for their data to be persisted.
The downside of this approach, is potential loss of the most recently written data upon crash. 
\remove{For example, 
if data is flushed to disk every $10$ seconds, then put operations executed in the last $10$ seconds before the crash 
may be undone. 

Nevertheless, \sys\ ensures 
}
It is required that \emph{data consistency} is preserved following recovery, in the sense that 
if some put is lost, all ensuing (and thus possibly dependent) puts are lost as well, and the recovery is to a well 
defined execution point some time before the crash.
 
\remove{
 \paragraph{Low write amplification.} 
 Minimizing disk writes is important not only for performance, but also for disk wear reduction.
}

\subsection{Design choices}

Given the aforementioned requirements, we make the following design choices in \sys:

\paragraph{Chunk-based organization.}
We organize data, both on-disk and in-memory,  in large chunks pertaining to  key ranges.  
%This enables efficient support of range scans, with  high cache locality and minimal loading of new memory pages. 
%Both the read and the write path go through chunks. 
Each chunk has a file representation called  \emph{funk}, and may be cached in a  memory \emph{munk} data structure.
This organization exploits spatial locality and is friendly to range scans.

We use a number of techniques to optimize in-memory  access, including partially sorting keys in each chunk and 
indexing munks. 
To expedite access to  keys whose chunks are only on-disk  (i.e., have no munks), 
individual popular keys are cached in a \emph{row cache}, 
and \emph{Bloom filters} are used to limit excessive access to disk. 

\paragraph{Infrequent disk compactions.} 
As long as a chunk is cached (has a munk), its funk's organization does not have to be optimized since 
queries do not access it. Therefore, \sys\ infrequently performs reorganization (compaction) on such funks.
Conversely, when a funk holds cold data, its organization hardly deteriorates, and therefore compaction is not necessary.
Note that this is unlike LSM-trees, where all disk components are compacted, regardless of which keys reside in memory and whether 
keys are hot or cold. 

\paragraph{Multi-versioning for atomic scans.} \sys\ employs multi-versioning along with
copy-on-write to keep data versions required by atomic scans. 
In other words, if a put attempts to overwrite a key  required by an active scan, then a new version is created alongside the 
existing one, whereas versions that are not needed by any scan are not retained. 
Thus, version management incurs a low overhead (as it occurs only on scans). 
%It also defines a simple rule for garbage collecting old versions.

\paragraph{In-funk WALs.} \sys\ logs writes within funks and refrains from duplicating the updates  in a separate WAL. This reduces write amplification and expedites recovery times. 
