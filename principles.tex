

\sys\ is a persistent key-value store supporting atomic \emph{put, get}, and  \emph{range scan} (or scan) operations. 
Scans are atomic in the sense that all values returned by a single scan belong to a consistent snapshot reflecting
the state of the data store at a unique point in time.



% \sys\ ensures \emph{durability} of all updates by writing updates to disk synchronously as part of the \emph{put} operation.

\sys\ is geared for high-throughput analytics applications, which (1) perform many range scans; 
(2) strive to accommodate the entire working set in RAM (for performance), and yet require  (3)
persistence with fast recovery. 
The key requirements from \sys\ are thus the following:
\begin{description}
%\item[Real-time updates] -- the system should allow updates to be processed at \inred{wire speed} with \inred{sub-millisecond} latency.  
\item[High performance with memory-resident working sets] -- 
To sustain high throughput, key-value stores leverage increasing memory sizes where they can hold most of the active data set. 
We strive for maximum performance in this ``high performance'' case.  
\item[Fast recovery to consistent state] --  Because crashes are inevitable, the mean-time-to-recover from crashes should be kept short.
Like other popular KV-stores~\cite{rocksdb,leveldb,hbase}, \sys\ supports \emph{asynchronous} persistence, 
where put requests are buffered in memory and periodically persisted to disk in bulk. 
This approach significantly reduces the latency of put operations, which do not wait for their data to be persisted.
The downside of this approach, is potential loss of the most recently written data upon crash. For example, 
if data is flushed to disk every $10$ seconds, then put operations executed in the last $10$ seconds before the crash 
may be undone. 
Nevertheless, \sys\ ensures that \emph{data consistency} is preserved following recovery, in the sense that 
if some put is lost, all ensuing (and thus possibly dependent) puts are lost as well, and the recovery is to a well 
defined execution point some time before the crash.
 
 \item[Fast atomic scans] -- 
 Analytics engines typically support multi-dimensional exploration of event data, which they realize in a key-value store using composite keys.   For example, a typical  KV-map used by an application like Flurry Analytics~\cite{flurry} 
  summarizes mobile traffic statistics grouped by a combination of dimensions like  date,  user-id, and application-id.
A query that retrieves data pertaining to a particular user thus needs to atomically 
retrieve the values pertaining to a range of keys. 
 To favor analytics queries, it is important to optimize  such scans. 
 \item[Low write amplification] -- Minimizing disk writes is important not only for performance, but also in order to reduce disk wear.

\end{description} 



Given these requirements, we make the following design choices in \sys:
\paragraph{Chunk-based organization.} We employ a memory organization and caching policy based on large \emph{chunks} of data pertaining to key ranges.  This allows us to support 
 range scans efficiently, with minimal indirection and loading of new memory pages. 
Both the read and the write path go through chunks. 
Each chunk has a file representation called  \emph{funk}, and may be cached 
in-memory in a  \emph{munk} data structure.
\paragraph{Fast in-memory search and traversal.} To expedite performance when 
most of the keys are cached in munks, 
we  optimize in-memory key access. We use a number of techniques for this purpose, including partially sorting keys in each chunk and 
indexing munks.
\paragraph{Multi-versioning and CoW for atomic scans.} \sys\ employs a copy-on-write policy to keep data versions required by atomic scans.
Thus, version management incurs a low overhead (as it occurs only on scans). It also defines a simple rule for garbage collecting old versions.
%\item[Lightweight puts] -- \sys\ allows put operations to proceed quickly by `dumping' their data to the appropriate file (and possibly) memory chunk; it amortizes the cost of optimizing the chunk's organization (by sorting and compaction) across many puts.  
\paragraph{Infrequent disk compactions.} 
As long as a chunk is cached (has a munk), its funk's organization does not have to be optimized since 
queries do not access it. Therefore, \sys\ does not perform reorganization (compaction) on such funks.
Conversely, when a funk holds cold data, its organization hardly deteriorates, and therefore compaction is not necessary.
Thus, \inred{disk compactions are rare in \sys.} 
Note that this is unlike LSM-trees, where all disk components are compacted, regardless of which keys reside in memory and whether 
keys are hot or cold. 

\paragraph{In-funk WALs.} \sys\ logs writes within funks and refrains from duplicating the updates  in a separate WAL. This reduces write amplification and expedites recovery times. 
 \Idit{We could say something about parallel I/O, but I think parallel I/O hurt us in the end, no?}
\remove{
to allow updates to proceed at a high throughput, we allow different threads to perform I/O to different files simultaneously.
We thus avoid I/O bottlenecks. % as in systems that employ a single write-ahead-log.
}