

%common lsm stores RocksDB, scylladb, HyperLevelDB, LevelDB, hbase, cassandra
The vast majority of industrial mainstream key-value stores are  implemented as LSM data
stores~\cite{Bigtable2008,hbase,cassandra2010,RocksDB, scylladb}, building on the foundation set by the original LSM-tree key-value store~\cite{O'Neil1996, Muth1998}.

Due to their popularity, much work has been dedicated in recent years for optimizing their implementation. Some work aims to increase performance of workloads that can be served from 
RAM by optimizing the LSM memory component~\cite{clsm2015, accordion}. Other focus on reducing write amplification and increasing throughput either by applying storage optimizations~\cite{WiscKey, PebblesDB}, defering compactions~\cite{PebblesDB, triad}, or by levereging the skewness of many workloads to tackle the problem earlier at the memory level~\cite{ accordion, triad}.
Specifically, PebblesDB~\cite{PebblesDB} introduces fragmented LSM tree structure which stores the leveled files in a skip-list like layout by dividing each level into guards of increasing granulrity. In contrast, \sys\ eliminates all leveling of the storage files. 
%Accordion~\cite{accordion} is a new write-path algorithm for LSM stores. 
Accordion~\cite{accordion} maintains a small dynamic memory component to absorb write operation. This active component is frequently merged with a larger immutable in-memory component by running in-memory flush and compaction at the background. This mechanism is somewhat similar to the rebalance mechanism used in \sys, however, \sys\ applies rebalance per chunk, while Accordion runs in-memory compaction for the entire store.

Unlike LSM-trees, Tucana~\cite{tucana} is a B-tree variant ($B^{\epsilon}$-tree). It uses per-leaf buffering to improve inserts and copy-on-write instead of WAL, trading compactions with smaller random I/O. 
Like \sys\ keys and values are appended to buffers and pointers to keys are sorted in contigouos segments, thereby having improved performance in read-dominated workloads.
Tucana uses an alternative approach to application level caching that is based on memory-mapped files (mmap). \sys\ on the other hand manages cache in memory data structures (munks and row cache).
%Split and merge update the index in a bottom up fashion. 
%Read and scan operations traverse the path from root to leaf. At the leaf, read operations perform binary search; scans subsequently iterate over the range of keys.
%In read dominated workloads Tucana benefits from contigouos key segments. However random I/O prevents Tucana from outperforming LSM trees in write dominated workloads.


In-memory kv-stores like~\cite{memcached, ignite, redis} are used for application data caching, but also as building blocks for distributed database with optional durability~\cite{ignite,redis}. These are not comparable with \sys\ as they are either not persistent~\cite{memcached}, do not support atomic scans~\cite{redis} or resemble  relational DBMS with a centralized WAL, B-tree index, and in-place updates~\cite{ignite}  more than a kv lsm store.